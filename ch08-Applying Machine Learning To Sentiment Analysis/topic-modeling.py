"""
ç”¨Latent Dirichlet Allocationå®ç°ä¸»é¢˜å»ºæ¨¡
Decomposing text documents with Latent Dirichlet Allocation
ä¸»é¢˜å»ºæ¨¡ï¼ˆtopic modelingï¼‰æè¿°äº†ä¸ºæ— æ ‡ç­¾æ–‡æœ¬æ–‡æ¡£åˆ†é…ä¸»é¢˜çš„ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯ä¸ºèšŠå¸æŒ‡å®šåˆ†ç±»æ ‡ç­¾ï¼Œå¯çœ‹ä½œæ˜¯èšç±»ä»»åŠ¡ï¼Œå±äºæ— ç›‘ç£å­¦ä¹ çš„å­ç±»
LDAæ¶‰åŠè®¸å¤šæ•°å­¦çŸ¥è¯†ï¼ŒåŒ…æ‹¬è´å¶æ–¯æ¨æ–­
    LDAæ˜¯ç”Ÿæˆæ¦‚ç‡æ¨¡å‹ï¼Œè¯•å›¾æ‰¾å‡ºç»å¸¸å‡ºç°åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„å•è¯ã€‚
    å‡è®¾æ¯ä¸ªæ–‡æ¡£éƒ½æ˜¯ç”±ä¸åŒå•è¯ç»„æˆçš„æ··åˆä½“ï¼Œé‚£äº›ç»å¸¸å‡ºç°çš„å•è¯å°±ä»£è¡¨ä¸»é¢˜ã€‚
    LDAçš„è¾“å…¥æ˜¯è¯è¢‹æ¨¡å‹ã€‚LDAæŠŠè¯è¢‹æ¨¡å‹ä½œä¸ºè¾“å…¥ï¼Œç„¶ååˆ†è§£ä¸ºä¸¤ä¸ªæ–°çŸ©é˜µï¼šæ–‡æ¡£ä¸»é¢˜çŸ©é˜µå’Œå•è¯ä¸»é¢˜çŸ©é˜µã€‚
    LDAå¿…é¡»æ‰‹åŠ¨å®šä¹‰çš„ä¸€ä¸ªè¶…å‚æ•°ï¼šä¸»é¢˜æ•°é‡
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

if __name__ == "__main__":
    df = pd.read_csv('movie_data.csv', encoding='utf-8')
    # ğŸ‘‡ ç”¨CountVectorizerç±»åˆ›å»ºè¯è¢‹çŸ©é˜µä»¥ä½œä¸ºLDAçš„è¾“å…¥ï¼Œmax_dfï¼šè¦è€ƒè™‘å•è¯çš„æœ€å¤§æ–‡æ¡£é¢‘ç‡ï¼Œmax_features: è¦è€ƒè™‘å•è¯çš„æ•°é‡é™åˆ¶ï¼Œå‡å¯è°ƒä¼˜
    count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)
    X = count.fit_transform(df['review'].values)
    lda = LatentDirichletAllocation(n_components=10,
                                    random_state=123,
                                    learning_method='batch',
                                    n_jobs=-1
                                    )
    X_topics = lda.fit_transform(X)
    print('lda components_.shape is {}'.format(lda.components_.shape))

    n_top_words = 5
    feature_names = count.get_feature_names()

    for topic_idx, topic in enumerate(lda.components_):
        print("Topic %d:" %(topic_idx+1))
        print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))