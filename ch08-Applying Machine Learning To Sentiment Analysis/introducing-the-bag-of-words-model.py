import os
import sys
import tarfile
import time
import urllib.request
import pyprind
import re

import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from nltk.stem.porter import PorterStemmer
import nltk
from nltk.corpus import stopwords
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

import gzip

from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
from distutils.version import LooseVersion as Version
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import LatentDirichletAllocation


def preprocessor(text):
    text = re.sub('<[^>]*>', '', text)
    emotions = re.findall('(?::|;|=)(?:-)?(?:\)|\(D|P)', text)
    text = (re.sub('[\W]+', ' ', text.lower()) + ' '.join(emotions).replace('-', ''))
    return text


def tokenizer(text):
    return text.split()


def tokenizer_porter(text):
    """æ³¢ç‰¹è¯å¹²æå–ç®—æ³•ï¼Œå¯èƒ½æ˜¯æœ€å¤è€ä¸”æœ€ç®€å•çš„è¯å¹²æå–ç®—æ³•ï¼Œå…¶ä»–å¸¸è§çš„è¯å¹²æå–ç®—æ³•åŒ…æ‹¬Snowball stemmerå’ŒLancaster stemmer
    """
    porter = PorterStemmer()
    return [porter.stem(word) for word in text.split()]


def remove_stop_words(text):
    """ä¸èƒ½ç”¨æ¥åŒºåˆ†ä¸åŒç±»åˆ«æ–‡æ¡£çš„æœ‰ç”¨ä¿¡æ¯çš„å•è¯ï¼Œå¦‚isï¼Œandï¼Œhasï¼Œlikeç­‰ï¼Œç§»é™¤åœç”¨è¯å¯èƒ½å¯¹å¤„ç†åŸå§‹æˆ–è€…å½’ä¸€åŒ–çš„è¯é¢‘è€Œétf-idfæœ‰ç›Šï¼Œå› ä¸ºå·²ç»é™ä½äº†é‚£äº›é¢‘ç¹
    å‡ºç°çš„å•è¯çš„æƒé‡ã€‚
    """
    from nltk.corpus import stopwords
    stop = stopwords.words('english')
    res = [w for w in tokenizer_porter(text) if w not in stop]
    return res


def introducing_BOW_model(df_t):
    """Introducing the bag-of-words model
    By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words
    model and transformed the following three sentences into sparse feature vectors:
        1. The sun is shining
        2. The weather is sweet
        3. The sun is shining, the weather is sweet, and one and one is two
    """
    # Transforming documents into feature vectors
    count = CountVectorizer()
    docs = np.array([
        'The sun is shining',
        'The weather is sweet',
        'The sun is shining, the weather is sweet, and one and one is two'])
    bag = count.fit_transform(docs)
    # print the contents of the vocabulary to get a better understanding of the underlying concepts
    print(count.vocabulary_)
    print(bag.toarray())
    np.set_printoptions(precision=2)

    # Scikit-learn implements the `TfidfTransformer`, that takes the raw term frequencies from
    # `CountVectorizer` as input and transforms them into tf-idfs:
    tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)
    print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

    tf_is = 3
    n_docs = 3
    idf_is = np.log((n_docs + 1) / (3 + 1))
    tfidf_is = tf_is * (idf_is + 1)
    print('tf_idf of term "is" = %.2f' % (tfidf_is))

    df['review_processed'] = df['review'].apply(preprocessor)
    # å‡†å¤‡å¥½ç”µå½±è¯„è®ºæ•°æ®é›†åï¼Œè€ƒè™‘å¦‚ä½•å°†æ–‡æœ¬è¯­æ–™åº“æ‹†åˆ†æˆç‹¬ç«‹çš„å…ƒç´ ï¼Œtokenizeæ–‡ä»¶çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡æŠŠæ¸…æ´—åçš„æ–‡æ¡£æ²¿ç©ºç™½å­—ç¬¦æ‹†åˆ†æˆå•ç‹¬çš„å•è¯


def lr_doc_cls(df_t):
    # Strip HTML and punctuation to speed up the GridSearch later
    from nltk.corpus import stopwords
    stop = stopwords.words('english')

    X_train = df_t.loc[:25000, 'review'].values
    y_train = df_t.loc[:25000, 'sentiment'].values
    X_test = df_t.loc[25000:, 'review'].values
    y_test = df_t.loc[25000:, 'sentiment'].values
    # ğŸ‘‡ ä½¿ç”¨TfidfVectorizer æ›¿æ¢ CountVectorizer å’Œ TfidfTransformerï¼Œå…¶ä¸­çš„TfidfTransformeråŒ…å«äº†è½¬æ¢å™¨å¯¹è±¡
    tf_idf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)
    param_grid = [{'vect__ngram_range': [(1, 1)],
                   'vect__stop_words': [stop, None],
                   'vect__tokenizer': [tokenizer, tokenizer_porter],
                   'clf__penalty': ['l1', 'l2'],
                   'clf__C': [1.0, 10.0, 100.0]},  # ğŸ‘ˆ é€†æ­£åˆ™åŒ–å‚æ•°Cï¼Œæ¯”è¾ƒæ­£åˆ™åŒ–å¼ºåº¦
                  {'vect__ngram_range': [(1, 1)],
                   'vect__stop_words': [stop, None],
                   'vect__tokenizer': [tokenizer, tokenizer_porter],
                   'vect__use_idf': [False],
                   'vect__norm': [None],
                   'clf__penalty': ['l1', 'l2'],
                   'clf__C': [1.0, 10.0, 100.0]},
                  ]
    lr_tfidf = Pipeline([('vect', tf_idf), ('clf', LogisticRegression(random_state=0, solver='liblinear'))])
    # ğŸ‘‡ ç”±äºç‰¹å¾å‘é‡å’Œè¯æ±‡å¾ˆå¤šï¼Œç½‘æ ¼æœç´¢çš„è®¡ç®—æˆæœ¬ç›¸å½“æ˜‚è´µï¼Œå› æ­¤é™åˆ¶äº†å‚æ•°ç»„åˆçš„æ•°é‡
    gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=-1)
    gs_lr_tfidf.fit(X_train, y_train)
    clf = gs_lr_tfidf.best_estimator_
    print('Best parameter set:%s' % gs_lr_tfidf.best_params_)
    print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)
    print('Test Accuracy: %.3f' % clf.score(X_test, y_test))


def illustrate_k_fold():
    np.random.seed(0)
    np.set_printoptions(precision=6)
    y = [np.random.randint(3) for i in range(25)]
    X = (y + np.random.randn(25)).reshape(-1, 1)
    cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=0).split(X, y))
    lr = LogisticRegression(random_state=123, multi_class='ovr', solver='lbfgs')
    score = cross_val_score(lr, X, y, cv=cv5_idx)
    print("cross_val_score:{}".format(score))
    """ By executing the code above, we created a simple data set of random integers that shall represent our class 
    labels. 
    Next, we fed the indices of 5 cross-validation folds (`cv3_idx`) to the `cross_val_score` scorer, 
    which returned 5 accuracy scores -- these are the 5 accuracy values for the 5 test folds.  
    Next, let us use the `GridSearchCV` object and feed it the same 5 cross-validation sets 
    (via the pre-generated `cv3_idx` indices): """
    lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)
    gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y)
    """
    As we can see, the scores for the 5 folds are exactly the same as the ones from `cross_val_score` earlier.
    Now, the best_score_ attribute of the `GridSearchCV` object, which becomes available after `fit`ting, returns the 
    average accuracy score of the best model: """
    print("gs.best_score is :{}".format(gs.best_score_))
    """ğŸ‘† As we can see, the result above is consistent with the average score computed the `cross_val_score`."""
    lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)
    score2 = cross_val_score(lr, X, y, cv=cv5_idx).mean()


if __name__ == "__main__":
    df = pd.read_csv('movie_data.csv', encoding='utf-8')
    # introducing_BOW_model(df)
    lr_doc_cls(df)
    """
    æ³¨æ„ï¼šlr_doc_clså‡½æ•°ä¸­ï¼Œ`gs_lr_tfidf.best_score_`æ˜¯k-æŠ˜å cross-validationå¾—åˆ†å‡å€¼ã€‚å¦‚æœobjectï¼š`GridSearchCV`æ˜¯5-foldäº¤å‰éªŒè¯ï¼ˆåƒä¸Š
    é¢çš„ä¾‹å­ï¼‰ï¼Œåˆ™"best_score_"è¿”å›çš„å¹³å‡åˆ†æ•°è¶…è¿‡æœ€ä½³æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„ä¾‹å­æ¥è§£é‡Š
    """
    illustrate_k_fold()
